{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction to Machine Learning: Final Project"
      ],
      "metadata": {
        "id": "RbY0G1T9Qh2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LÃ©o Dupire | ld2494"
      ],
      "metadata": {
        "id": "1537dgoEQrjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Data"
      ],
      "metadata": {
        "id": "hJjIs9xzDn8i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRUNHU8srA5b"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/drive/MyDrive/NYU"
      ],
      "metadata": {
        "id": "0tI0-SABrvTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch\n",
        "# !pip install torchvision\n",
        "# !pip install opencv-python\n",
        "# !pip install pickle-mixin\n",
        "# !pip install pandas\n",
        "# !pip install matplotlib\n",
        "# !pip install sklearn\n",
        "# !pip install -U scikit-learn scipy matplotlib\n",
        "# !pip install statsmodels"
      ],
      "metadata": {
        "id": "-N6a8WfRim_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pickle as pkl\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "7GoI2oMatasw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LazyLoadDataset(Dataset):\n",
        "  def __init__(self, path, train=True, transform=None):\n",
        "    self.transform = transform\n",
        "    self.train = train\n",
        "    path = path + (\"train/\" if train else \"test/\")\n",
        "\n",
        "    self.pathX = path + \"X/\"\n",
        "    self.pathY = path + \"Y/\"\n",
        "\n",
        "    self.data = os.listdir(self.pathX)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    f = self.data[idx]\n",
        "\n",
        "    img0 = cv2.imread(self.pathX + f + \"/rgb/0.png\")\n",
        "    img1 = cv2.imread(self.pathX + f + \"/rgb/1.png\")\n",
        "    img2 = cv2.imread(self.pathX + f + \"/rgb/2.png\")\n",
        "\n",
        "    if self.transform is not None:\n",
        "      img0 = self.transform(img0)\n",
        "      img1 = self.transform(img1)\n",
        "      img2 = self.transform(img2)\n",
        "\n",
        "    depth = np.load(self.pathX + f + \"/depth.npy\")\n",
        "\n",
        "    field_id = pkl.load(open(self.pathX + f + \"/field_id.pkl\", \"rb\"))\n",
        "\n",
        "    Y = None\n",
        "    if self.train:\n",
        "      Y = np.load(self.pathY + f + \".npy\")\n",
        "\n",
        "    return (img0, img1, img2, depth, int(field_id)), Y\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data_folders)"
      ],
      "metadata": {
        "id": "34du_-fLrTA2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train = LazyLoadDataset(\"./lazydata/\")\n",
        "# test = LazyLoadDataset(\"./lazydata/\", train=False)"
      ],
      "metadata": {
        "id": "3wzeCm5xs14O"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define custom transform function\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "74HUuHadS22v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Download Data (Only Once)"
      ],
      "metadata": {
        "id": "UcQ2pG2SF-l1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X_img = []\n",
        "# X_depth = []\n",
        "# y = []\n",
        "\n",
        "# for i in range(500): #len(train.data)):\n",
        "#   print(i) \n",
        "#   (temp_img0, temp_img1, temp_img2, temp_depth, temp_field_id), temp_Y = train[i]\n",
        "\n",
        "#   X_img.append(np.array(transform(temp_img0)))\n",
        "#   # X_depth.append(temp_depth)\n",
        "#   y.append(temp_Y)"
      ],
      "metadata": {
        "id": "cg7zx5FvCaHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(X_img, \"X_img.pt\")\n",
        "# torch.save(y, \"y.pt\")"
      ],
      "metadata": {
        "id": "3sBHEF5Icc5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load Data (If Needed)"
      ],
      "metadata": {
        "id": "3CI7K36Jy02-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_X_FULL = np.array(torch.load('./Desktop/Data_ML/train_X_FULL.pt'))\n",
        "# train_X1_FULL = np.array(torch.load('./Desktop/Data_ML/train_X1_FULL.pt'))\n",
        "# train_X2_FULL = np.array(torch.load('./Desktop/Data_ML/train_X2_FULL.pt'))\n",
        "train_DEPTH_FULL = np.array(torch.load('./Desktop/Data_ML/train_DEPTH_FULL.pt'))\n",
        "train_y_FULL = np.array(torch.load('./Desktop/Data_ML/train_y_FULL.pt'))"
      ],
      "metadata": {
        "id": "injSDYFyF04z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These will go into the model(s)\n",
        "train_img = torch.tensor(np.array(train_X_FULL).reshape([len(train_X_FULL), 3, 224, 224]))\n",
        "# train_img1 = torch.tensor(np.array(train_X1_FULL).reshape([len(train_X_FULL), 3, 224, 224]))\n",
        "# train_img2 = torch.tensor(np.array(train_X2_FULL).reshape([len(train_X_FULL), 3, 224, 224]))\n",
        "train_depth = torch.tensor(np.array(train_DEPTH_FULL).reshape([len(train_X_FULL), 3, 224, 224]))\n",
        "train_Y = torch.tensor(np.array(train_y_FULL))"
      ],
      "metadata": {
        "id": "k-bEnwA9MLRd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_img.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJDuBOU5rL04",
        "outputId": "a0b01a30-8dc6-4b6a-af03-c77b5f6f8d7e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3396, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model"
      ],
      "metadata": {
        "id": "cVTKnf4HXaKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a CNN class\n",
        "class ConvNeuralNet(nn.Module):\n",
        "\t#  Determine what layers and their order in CNN object \n",
        "    def __init__(self, num_classes):\n",
        "        super(ConvNeuralNet, self).__init__()\n",
        "        \n",
        "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
        "\n",
        "        self.conv_layer2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
        "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        \n",
        "        self.conv_layer3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "        self.conv_layer4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
        "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(179776, 512)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(128, 12)\n",
        "    \n",
        "    # Progresses data across layers    \n",
        "    def forward(self, x):\n",
        "        out = self.conv_layer1(x)\n",
        "\n",
        "        out = self.conv_layer2(out)\n",
        "        out = self.max_pool1(out)\n",
        "        out = self.bn1(out.reshape(1, 32, 110, 110))\n",
        "        \n",
        "        out = self.conv_layer3(out)\n",
        "        \n",
        "        out = self.conv_layer4(out)\n",
        "        out = self.max_pool2(out)\n",
        "\n",
        "        # Flatten manually\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = out.reshape(-1)\n",
        "        \n",
        "        out = self.fc1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.fc3(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "kAI8N-yHVfNo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define relevant variables for the ML task\n",
        "batch_size = 64\n",
        "num_classes = 12\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "train_size = len(train_img)\n",
        "\n",
        "# Model for top RGB Image\n",
        "model_RGBTop = ConvNeuralNet(num_classes)\n",
        "\n",
        "# Model for Depth Image\n",
        "model_DEPTH = ConvNeuralNet(num_classes)\n",
        "\n",
        "# Set Loss function with criterion + criterion\n",
        "criterion = nn.MSELoss()\n",
        "optimizer_RGBTop = torch.optim.SGD(model_RGBTop.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
        "optimizer_DEPTH = torch.optim.SGD(model_DEPTH.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  "
      ],
      "metadata": {
        "id": "RaycVdLyVfPT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train RGB Top Image Model"
      ],
      "metadata": {
        "id": "Gr7GR5INpdI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomize order of images\n",
        "order = list(range(len(train_img)))\n",
        "random.shuffle(order)\n",
        "\n",
        "# We use the pre-defined number of epochs to determine how many iterations to train the network on\n",
        "for epoch in range(num_epochs):\n",
        "\t#Load in the data in batches using the train_loader object\n",
        "    for i in range(train_size):\n",
        "        if (i%10 == 0):\n",
        "          print(i, end=\" \")\n",
        "\n",
        "        # Move tensors to the configured device\n",
        "        images = train_img[order[i]]\n",
        "        labels = train_Y[order[i]]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_RGBTop(images.float())\n",
        "        loss = torch.sqrt(criterion(outputs, labels.float()))\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer_RGBTop.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_RGBTop.step()\n",
        "\n",
        "    print('\\nEpoch [{}/{}], Loss: {:.6f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxJoeOvNJaxA",
        "outputId": "e80f2556-441c-4b20-8cbf-398b82e26a52"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 2260 2270 2280 2290 2300 2310 2320 2330 2340 2350 2360 2370 2380 2390 2400 2410 2420 2430 2440 2450 2460 2470 2480 2490 2500 2510 2520 2530 2540 2550 2560 2570 2580 2590 2600 2610 2620 2630 2640 2650 2660 2670 2680 2690 2700 2710 2720 2730 2740 2750 2760 2770 2780 2790 2800 2810 2820 2830 2840 2850 2860 2870 2880 2890 2900 2910 2920 2930 2940 2950 2960 2970 2980 2990 3000 3010 3020 3030 3040 3050 3060 3070 3080 3090 3100 3110 3120 3130 3140 3150 3160 3170 3180 3190 3200 3210 3220 3230 3240 3250 3260 3270 3280 3290 3300 3310 3320 3330 3340 3350 3360 3370 3380 3390 \n",
            "Epoch [1/10], Loss: 0.006308\n",
            "\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 2260 2270 2280 2290 2300 2310 2320 2330 2340 2350 2360 2370 2380 2390 2400 2410 2420 2430 2440 2450 2460 2470 2480 2490 2500 2510 2520 2530 2540 2550 2560 2570 2580 2590 2600 2610 2620 2630 2640 2650 2660 2670 2680 2690 2700 2710 2720 2730 2740 2750 2760 2770 2780 2790 2800 2810 2820 2830 2840 2850 2860 2870 2880 2890 2900 2910 2920 2930 2940 2950 2960 2970 2980 2990 3000 3010 3020 3030 3040 3050 3060 3070 3080 3090 3100 3110 3120 3130 3140 3150 3160 3170 3180 3190 3200 3210 3220 3230 3240 3250 3260 3270 3280 3290 3300 3310 3320 3330 3340 3350 3360 3370 3380 3390 \n",
            "Epoch [2/10], Loss: 0.004221\n",
            "\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 2260 2270 2280 2290 2300 2310 2320 2330 2340 2350 2360 2370 2380 2390 2400 2410 2420 2430 2440 2450 2460 2470 2480 2490 2500 2510 2520 2530 2540 2550 2560 2570 2580 2590 2600 2610 2620 2630 2640 2650 2660 2670 2680 2690 2700 2710 2720 2730 2740 2750 2760 2770 2780 2790 2800 2810 2820 2830 2840 2850 2860 2870 2880 2890 2900 2910 2920 2930 2940 2950 2960 2970 2980 2990 3000 3010 3020 3030 3040 3050 3060 3070 3080 3090 3100 3110 3120 3130 3140 3150 3160 3170 3180 3190 3200 3210 3220 3230 3240 3250 3260 3270 3280 3290 3300 3310 3320 3330 3340 3350 3360 3370 3380 3390 \n",
            "Epoch [3/10], Loss: 0.004221\n",
            "\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 2260 2270 2280 2290 2300 2310 2320 2330 2340 2350 2360 2370 2380 2390 2400 2410 2420 2430 2440 2450 2460 2470 2480 2490 2500 2510 2520 2530 2540 2550 2560 2570 2580 2590 2600 2610 2620 2630 2640 2650 2660 2670 2680 2690 2700 2710 2720 2730 2740 2750 2760 2770 2780 2790 2800 2810 2820 2830 2840 2850 2860 2870 2880 2890 2900 2910 2920 2930 2940 2950 2960 2970 2980 2990 3000 3010 3020 3030 3040 3050 3060 3070 3080 3090 3100 3110 3120 3130 3140 3150 3160 3170 3180 3190 3200 3210 3220 3230 3240 3250 3260 3270 3280 3290 3300 3310 3320 3330 3340 3350 3360 3370 3380 3390 \n",
            "Epoch [4/10], Loss: 0.004079\n",
            "\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 2260 2270 2280 2290 2300 2310 2320 2330 2340 2350 2360 2370 2380 2390 2400 2410 2420 2430 2440 2450 2460 2470 2480 2490 2500 2510 2520 2530 2540 2550 2560 2570 2580 2590 2600 2610 2620 2630 2640 2650 2660 2670 2680 2690 2700 2710 2720 2730 2740 2750 2760 2770 2780 2790 2800 2810 2820 2830 2840 2850 2860 2870 2880 2890 2900 2910 2920 2930 2940 2950 2960 2970 2980 2990 3000 3010 3020 3030 3040 3050 3060 3070 3080 3090 3100 3110 3120 3130 3140 3150 3160 3170 3180 3190 3200 3210 3220 3230 3240 3250 3260 3270 3280 3290 3300 3310 3320 3330 3340 3350 3360 3370 3380 3390 \n",
            "Epoch [5/10], Loss: 0.003745\n",
            "\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 2260 2270 2280 2290 2300 2310 2320 2330 2340 2350 2360 2370 2380 2390 2400 2410 2420 2430 2440 2450 2460 2470 2480 2490 2500 2510 2520 2530 2540 2550 2560 2570 2580 2590 2600 2610 2620 2630 2640 2650 2660 2670 2680 2690 2700 2710 2720 2730 2740 2750 2760 2770 2780 2790 2800 2810 2820 2830 2840 2850 2860 2870 2880 2890 2900 2910 2920 2930 2940 2950 2960 2970 2980 2990 3000 3010 3020 3030 3040 3050 3060 3070 3080 3090 3100 3110 3120 3130 3140 3150 3160 3170 3180 3190 3200 3210 3220 3230 3240 3250 3260 3270 3280 3290 3300 3310 3320 3330 3340 3350 3360 3370 3380 3390 \n",
            "Epoch [6/10], Loss: 0.003670\n",
            "\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 2260 2270 2280 2290 2300 2310 2320 2330 2340 2350 2360 2370 2380 2390 2400 2410 2420 2430 2440 2450 2460 2470 2480 2490 2500 2510 2520 2530 2540 2550 2560 2570 2580 2590 2600 2610 2620 2630 2640 2650 2660 2670 2680 2690 2700 2710 2720 2730 2740 2750 2760 2770 2780 2790 2800 2810 2820 2830 2840 2850 2860 2870 2880 2890 2900 2910 2920 2930 2940 2950 2960 2970 2980 2990 3000 3010 3020 3030 3040 3050 3060 3070 3080 3090 3100 3110 3120 3130 3140 3150 3160 3170 3180 3190 3200 3210 3220 3230 3240 3250 3260 3270 3280 3290 3300 3310 3320 3330 3340 3350 3360 3370 3380 3390 \n",
            "Epoch [7/10], Loss: 0.003690\n",
            "\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 2260 2270 2280 2290 2300 2310 2320 2330 2340 2350 2360 2370 2380 2390 2400 2410 2420 2430 2440 2450 2460 2470 2480 2490 2500 2510 2520 2530 2540 2550 2560 2570 2580 2590 2600 2610 2620 2630 2640 2650 2660 2670 2680 2690 2700 2710 2720 2730 2740 2750 2760 2770 2780 2790 2800 2810 2820 2830 2840 2850 2860 2870 2880 2890 2900 2910 2920 2930 2940 2950 2960 2970 2980 2990 3000 3010 3020 3030 3040 3050 3060 3070 3080 3090 3100 3110 3120 3130 3140 3150 3160 3170 3180 3190 3200 3210 3220 3230 3240 3250 3260 3270 3280 3290 3300 3310 3320 3330 3340 3350 3360 3370 3380 3390 \n",
            "Epoch [8/10], Loss: 0.003863\n",
            "\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 2260 2270 2280 2290 2300 2310 2320 2330 2340 2350 2360 2370 2380 2390 2400 2410 2420 2430 2440 2450 2460 2470 2480 2490 2500 2510 2520 2530 2540 2550 2560 2570 2580 2590 2600 2610 2620 2630 2640 2650 2660 2670 2680 2690 2700 2710 2720 2730 2740 2750 2760 2770 2780 2790 2800 2810 2820 2830 2840 2850 2860 2870 2880 2890 2900 2910 2920 2930 2940 2950 2960 2970 2980 2990 3000 3010 3020 3030 3040 3050 3060 3070 3080 3090 3100 3110 3120 3130 3140 3150 3160 3170 3180 3190 3200 3210 3220 3230 3240 3250 3260 3270 3280 3290 3300 3310 3320 3330 3340 3350 3360 3370 3380 3390 \n",
            "Epoch [9/10], Loss: 0.003878\n",
            "\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 2260 2270 2280 2290 2300 2310 2320 2330 2340 2350 2360 2370 2380 2390 2400 2410 2420 2430 2440 2450 2460 2470 2480 2490 2500 2510 2520 2530 2540 2550 2560 2570 2580 2590 2600 2610 2620 2630 2640 2650 2660 2670 2680 2690 2700 2710 2720 2730 2740 2750 2760 2770 2780 2790 2800 2810 2820 2830 2840 2850 2860 2870 2880 2890 2900 2910 2920 2930 2940 2950 2960 2970 2980 2990 3000 3010 3020 3030 3040 3050 3060 3070 3080 3090 3100 3110 3120 3130 3140 3150 3160 3170 3180 3190 3200 3210 3220 3230 3240 3250 3260 3270 3280 3290 3300 3310 3320 3330 3340 3350 3360 3370 3380 3390 \n",
            "Epoch [10/10], Loss: 0.004025\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train Depth Model"
      ],
      "metadata": {
        "id": "e4cbUUtbpj9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the pre-defined number of epochs to determine how many iterations to train the network on\n",
        "for epoch in range(num_epochs):\n",
        "\t#Load in the data in batches using the train_loader object\n",
        "    for i in range(train_size):\n",
        "        if (i%10 == 0):\n",
        "          print(i, end=\" \")\n",
        "\n",
        "        # Move tensors to the configured device\n",
        "        images = train_depth[order[i]]\n",
        "        labels = train_Y[order[i]]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_DEPTH(images.float())\n",
        "        loss = torch.sqrt(criterion(outputs, labels.float()))\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer_DEPTH.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_DEPTH.step()\n",
        "\n",
        "    print('\\nEpoch [{}/{}], Loss: {:.6f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCKECrsnpoUZ",
        "outputId": "22bf0f6b-be48-4d26-f02f-02e5eb53cbdb"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 2260 2270 2280 2290 2300 2310 2320 2330 2340 2350 2360 2370 2380 2390 2400 2410 2420 2430 2440 2450 2460 2470 2480 2490 2500 2510 2520 2530 2540 2550 2560 2570 2580 2590 2600 2610 2620 2630 2640 2650 2660 2670 2680 2690 2700 2710 2720 2730 2740 2750 2760 2770 2780 2790 2800 2810 2820 2830 2840 2850 2860 2870 2880 2890 2900 2910 2920 2930 2940 2950 2960 2970 2980 2990 3000 3010 3020 3030 3040 3050 3060 3070 3080 3090 3100 3110 3120 3130 3140 3150 3160 3170 3180 3190 3200 3210 3220 3230 3240 3250 3260 3270 3280 3290 3300 3310 3320 3330 3340 3350 3360 3370 3380 3390 \n",
            "Epoch [1/5], Loss: 0.012164\n",
            "\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 2260 2270 2280 2290 2300 2310 2320 2330 2340 2350 2360 2370 2380 2390 2400 2410 2420 2430 2440 2450 2460 2470 2480 2490 2500 2510 2520 2530 2540 2550 2560 2570 2580 2590 2600 2610 2620 2630 2640 2650 2660 2670 2680 2690 2700 2710 2720 2730 2740 2750 2760 2770 2780 2790 2800 2810 2820 2830 2840 2850 2860 2870 2880 2890 2900 2910 2920 2930 2940 2950 2960 2970 2980 2990 3000 3010 3020 3030 3040 3050 3060 3070 3080 3090 3100 3110 3120 3130 3140 3150 3160 3170 3180 3190 3200 3210 3220 3230 3240 3250 3260 3270 3280 3290 3300 3310 3320 3330 3340 3350 3360 3370 3380 3390 \n",
            "Epoch [2/5], Loss: 0.009471\n",
            "\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 2260 2270 2280 2290 2300 2310 2320 2330 2340 2350 2360 2370 2380 2390 2400 2410 2420 2430 2440 2450 2460 2470 2480 2490 2500 2510 2520 2530 2540 2550 2560 2570 2580 2590 2600 2610 2620 2630 2640 2650 2660 2670 2680 2690 2700 2710 2720 2730 2740 2750 2760 2770 2780 2790 2800 2810 2820 2830 2840 2850 2860 2870 2880 2890 2900 2910 2920 2930 2940 2950 2960 2970 2980 2990 3000 3010 3020 3030 3040 3050 3060 3070 3080 3090 3100 3110 3120 3130 3140 3150 3160 3170 3180 3190 3200 3210 3220 3230 3240 3250 3260 3270 3280 3290 3300 3310 3320 3330 3340 3350 3360 3370 3380 3390 \n",
            "Epoch [3/5], Loss: 0.008312\n",
            "\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 2260 2270 2280 2290 2300 2310 2320 2330 2340 2350 2360 2370 2380 2390 2400 2410 2420 2430 2440 2450 2460 2470 2480 2490 2500 2510 2520 2530 2540 2550 2560 2570 2580 2590 2600 2610 2620 2630 2640 2650 2660 2670 2680 2690 2700 2710 2720 2730 2740 2750 2760 2770 2780 2790 2800 2810 2820 2830 2840 2850 2860 2870 2880 2890 2900 2910 2920 2930 2940 2950 2960 2970 2980 2990 3000 3010 3020 3030 3040 3050 3060 3070 3080 3090 3100 3110 3120 3130 3140 3150 3160 3170 3180 3190 3200 3210 3220 3230 3240 3250 3260 3270 3280 3290 3300 3310 3320 3330 3340 3350 3360 3370 3380 3390 \n",
            "Epoch [4/5], Loss: 0.007321\n",
            "\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 850 860 870 880 890 900 910 920 930 940 950 960 970 980 990 1000 1010 1020 1030 1040 1050 1060 1070 1080 1090 1100 1110 1120 1130 1140 1150 1160 1170 1180 1190 1200 1210 1220 1230 1240 1250 1260 1270 1280 1290 1300 1310 1320 1330 1340 1350 1360 1370 1380 1390 1400 1410 1420 1430 1440 1450 1460 1470 1480 1490 1500 1510 1520 1530 1540 1550 1560 1570 1580 1590 1600 1610 1620 1630 1640 1650 1660 1670 1680 1690 1700 1710 1720 1730 1740 1750 1760 1770 1780 1790 1800 1810 1820 1830 1840 1850 1860 1870 1880 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010 2020 2030 2040 2050 2060 2070 2080 2090 2100 2110 2120 2130 2140 2150 2160 2170 2180 2190 2200 2210 2220 2230 2240 2250 2260 2270 2280 2290 2300 2310 2320 2330 2340 2350 2360 2370 2380 2390 2400 2410 2420 2430 2440 2450 2460 2470 2480 2490 2500 2510 2520 2530 2540 2550 2560 2570 2580 2590 2600 2610 2620 2630 2640 2650 2660 2670 2680 2690 2700 2710 2720 2730 2740 2750 2760 2770 2780 2790 2800 2810 2820 2830 2840 2850 2860 2870 2880 2890 2900 2910 2920 2930 2940 2950 2960 2970 2980 2990 3000 3010 3020 3030 3040 3050 3060 3070 3080 3090 3100 3110 3120 3130 3140 3150 3160 3170 3180 3190 3200 3210 3220 3230 3240 3250 3260 3270 3280 3290 3300 3310 3320 3330 3340 3350 3360 3370 3380 3390 \n",
            "Epoch [5/5], Loss: 0.007174\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###RGBD Model"
      ],
      "metadata": {
        "id": "Qr_1fohZcyiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge two depth 3 images into one of depth 6\n",
        "train_merge = torch.cat((train_img, train_depth), 1)"
      ],
      "metadata": {
        "id": "NXmNGT0Qc3hq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_merge.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftAcHbCMdCEA",
        "outputId": "ad676ff6-a9ac-4100-cd68-84cc78977ab9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3396, 6, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a CNN class\n",
        "class RGBD(nn.Module):\n",
        "\t#  Determine what layers and their order in CNN object \n",
        "    def __init__(self, num_classes):\n",
        "        super(RGBD, self).__init__()\n",
        "        \n",
        "        self.conv_layer1 = nn.Conv2d(in_channels=6, out_channels=64, kernel_size=4)\n",
        "\n",
        "        self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
        "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        self.conv_layer3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
        "        self.conv_layer4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3)\n",
        "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(346112, 512)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(128, 12)\n",
        "    \n",
        "    # Progresses data across layers    \n",
        "    def forward(self, x):\n",
        "        out = self.conv_layer1(x)\n",
        "\n",
        "        out = self.conv_layer2(out)\n",
        "        out = self.max_pool1(out)\n",
        "        out = self.bn1(out.reshape(1, 64, 109, 109))\n",
        "        \n",
        "        out = self.conv_layer3(out)\n",
        "        \n",
        "        out = self.conv_layer4(out)\n",
        "        out = self.max_pool2(out)\n",
        "\n",
        "        # Flatten manually\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = out.reshape(-1)\n",
        "        \n",
        "        out = self.fc1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.fc3(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "YvIOUNiGSkoh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define relevant variables for the ML task\n",
        "batch_size = 64\n",
        "num_classes = 12\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "train_size = 500 #len(train_img)\n",
        "\n",
        "# Model for top RGB Image\n",
        "model_RGBD = RGBD(num_classes)\n",
        "\n",
        "# Set Loss function with criterion + criterion\n",
        "criterion = nn.MSELoss()\n",
        "optimizer_RGBD = torch.optim.SGD(model_RGBD.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  "
      ],
      "metadata": {
        "id": "qPes6ihVgfMP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomize order of images\n",
        "order = list(range(len(train_img)))\n",
        "random.shuffle(order)\n",
        "\n",
        "# We use the pre-defined number of epochs to determine how many iterations to train the network on\n",
        "for epoch in range(num_epochs):\n",
        "\t#Load in the data in batches using the train_loader object\n",
        "    for i in range(train_size):\n",
        "        if (i%10 == 0):\n",
        "          print(i, end=\" \")\n",
        "\n",
        "        # Move tensors to the configured device\n",
        "        images = train_merge[order[i]]\n",
        "        labels = train_Y[order[i]]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_RGBD(images.float())\n",
        "        loss = torch.sqrt(criterion(outputs, labels.float()))\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer_RGBD.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_RGBD.step()\n",
        "\n",
        "    print('\\nEpoch [{}/{}], Loss: {:.6f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrgxAqMigtrs",
        "outputId": "826e3cb6-1474-4d82-9279-b8a3fb80349c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 \n",
            "Epoch [1/5], Loss: 0.008423\n",
            "\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 \n",
            "Epoch [2/5], Loss: 0.007043\n",
            "\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 \n",
            "Epoch [3/5], Loss: 0.006709\n",
            "\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 \n",
            "Epoch [4/5], Loss: 0.006309\n",
            "\n",
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 \n",
            "Epoch [5/5], Loss: 0.003951\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train Merger Model"
      ],
      "metadata": {
        "id": "rpo5bOShwsDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####First get the predictions from the RGBTop and Depth models for the full train dataset"
      ],
      "metadata": {
        "id": "2McRxAsAw9s-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RGBTop_pred = []\n",
        "DEPTH_pred = []\n",
        "with torch.no_grad():\n",
        "    for i in range(len(train_img)):\n",
        "        if (i%10 == 0):\n",
        "          print(i, end=\" \")\n",
        "        IMG = train_img[i]\n",
        "        DPT = train_depth[i]\n",
        "\n",
        "        outputs_IMG = model_RGBTop(IMG)\n",
        "        outputs_DPT = model_DEPTH(DPT)\n",
        "\n",
        "        RGBTop_pred.append(outputs_IMG)\n",
        "        DEPTH_pred.append(outputs_DPT)"
      ],
      "metadata": {
        "id": "p6ljIPj5wxD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RGBTop = np.array(pd.DataFrame.from_records(np.array(RGBTop_pred))).astype(float)\n",
        "DEPTH = np.array(pd.DataFrame.from_records(np.array(DEPTH_pred))).astype(float)"
      ],
      "metadata": {
        "id": "hU7J8TF6DmKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_data = []\n",
        "\n",
        "for i in range(len(train_img)):\n",
        "  temp = []\n",
        "  for j in range(12):\n",
        "    temp.append([RGBTop[i][j], DEPTH[i][j]])\n",
        "  merge_data.append(np.array(temp).T)\n",
        "\n",
        "merge_data = torch.tensor(merge_data).reshape([3396, 1, 2, 12])"
      ],
      "metadata": {
        "id": "vQNrdxbC1Lui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA4sxIxuJR_0",
        "outputId": "f9c39e23-ed40-4917-fd5a-abd3ad52f5b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3396, 1, 2, 12])"
            ]
          },
          "metadata": {},
          "execution_count": 529
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MergerCNN(nn.Module):\n",
        "\t#  Determine what layers and their order in CNN object \n",
        "    def __init__(self, num_classes):\n",
        "        super(MergerCNN, self).__init__()\n",
        "        \n",
        "        # Add more neurons to prevent 'unlearning'\n",
        "        self.conv_layer1 = nn.Conv2d(in_channels=1, out_channels=100, kernel_size=3, padding=1) \n",
        "        self.batch_norm1 = nn.BatchNorm2d(100)\n",
        "        \n",
        "        self.conv_layer2 = nn.Conv2d(in_channels=100, out_channels=200, kernel_size=1)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(200)\n",
        "        self.relu1 = nn.LeakyReLU(0.1)\n",
        "        \n",
        "        self.conv_layer3 = nn.Conv2d(in_channels=200, out_channels=200, kernel_size=1)\n",
        "        self.batch_norm3 = nn.BatchNorm2d(200)\n",
        "        # self.dropout2 = nn.Dropout(p=0.1)\n",
        "        self.relu2 = nn.LeakyReLU(0.1)\n",
        "\n",
        "        self.fc1 = nn.Linear(12, 64)\n",
        "        self.relu3 = nn.LeakyReLU(0.1)\n",
        "        self.fc2 = nn.Linear(64, 12)\n",
        "    \n",
        "    # Progresses data across layers    \n",
        "    def forward(self, x):\n",
        "        out = self.conv_layer1(x)\n",
        "        out = self.batch_norm1(out.reshape([1, 100, 2, 12]))\n",
        "\n",
        "        out = self.conv_layer2(out)\n",
        "        out = self.batch_norm2(out)\n",
        "        out = self.relu1(out)\n",
        "        \n",
        "        out = self.conv_layer3(out)\n",
        "        out = self.batch_norm3(out)\n",
        "        # out = self.dropout2(out)\n",
        "        out = self.relu2(out)\n",
        " \n",
        "        out = self.fc1(out)\n",
        "        out = self.relu3(out)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "TbKAiYV6yf5t"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define relevant variables for the ML task\n",
        "num_classes = 12\n",
        "learning_rate = 0.001\n",
        "num_epochs = 30\n",
        "train_size = len(train_img)\n",
        "\n",
        "# Model for top RGB Image\n",
        "model_MERGE = MergerCNN(num_classes)\n",
        "\n",
        "# Set Loss function with criterion + criterion\n",
        "criterion = nn.MSELoss()\n",
        "optimizer_MERGE = torch.optim.SGD(model_MERGE.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9) "
      ],
      "metadata": {
        "id": "R4nYXtp1zuJZ"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the pre-defined number of epochs to determine how many iterations to train the network on\n",
        "for epoch in range(num_epochs):\n",
        "\t#Load in the data in batches using the train_loader object\n",
        "    for i in range(train_size):\n",
        "        if (i%100 == 0):\n",
        "          print(i, end=\" \")\n",
        "\n",
        "        # Move tensors to the configured device\n",
        "        images = merge_data[order[i]]\n",
        "        labels = train_Y[order[i]]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model_MERGE(images.float())\n",
        "        loss = torch.sqrt(criterion(outputs, labels.float()))\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer_MERGE.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_MERGE.step()\n",
        "\n",
        "    print('\\nEpoch [{}/{}], Loss: {:.6f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3tG6NEn0m96",
        "outputId": "b07f707c-623d-4152-9ecc-d4ddbed020b1"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/leodupire/miniforge3/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([1, 512, 2, 12])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [1/30], Loss: 0.025206\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [2/30], Loss: 0.024033\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [3/30], Loss: 0.023953\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [4/30], Loss: 0.024014\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [5/30], Loss: 0.024016\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [6/30], Loss: 0.023912\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [7/30], Loss: 0.023547\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [8/30], Loss: 0.022958\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [9/30], Loss: 0.022650\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [10/30], Loss: 0.022480\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [11/30], Loss: 0.021649\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [12/30], Loss: 0.020608\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [13/30], Loss: 0.020333\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [14/30], Loss: 0.020296\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [15/30], Loss: 0.020390\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [16/30], Loss: 0.020537\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [17/30], Loss: 0.020694\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [18/30], Loss: 0.020870\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [19/30], Loss: 0.021054\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [20/30], Loss: 0.021241\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [21/30], Loss: 0.021416\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [22/30], Loss: 0.021577\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [23/30], Loss: 0.021730\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [24/30], Loss: 0.021872\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [25/30], Loss: 0.022006\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [26/30], Loss: 0.022125\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [27/30], Loss: 0.022243\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [28/30], Loss: 0.022350\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [29/30], Loss: 0.022451\n",
            "\n",
            "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 2300 2400 2500 2600 2700 2800 2900 3000 3100 3200 3300 \n",
            "Epoch [30/30], Loss: 0.022542\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation"
      ],
      "metadata": {
        "id": "C8ve6IjgsfAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Download Data (Only Once)"
      ],
      "metadata": {
        "id": "m9NFXjOUGvKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_img = []\n",
        "# test_depth = []\n",
        "# test_ID = []\n",
        "\n",
        "# for i in range(len(test.data)):\n",
        "#   print(i)\n",
        "#   (temp_img0, temp_img1, temp_img2, temp_depth, temp_field_id), temp_Y = test[i]\n",
        "\n",
        "#   test_img.append(np.array(transform(temp_img0)))\n",
        "#   # X_depth.append(temp_depth)\n",
        "#   test_ID.append(temp_field_id)"
      ],
      "metadata": {
        "id": "m3YyineAu3_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(test_img, \"test_img.pt\")"
      ],
      "metadata": {
        "id": "F3XoMsodwJly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load Data (If Needed)"
      ],
      "metadata": {
        "id": "D59vxWih1uVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_imgs = torch.tensor(np.array(torch.load('./Desktop/Data_ML/test_IMG.pt')))\n",
        "test_depth = torch.tensor(np.array(torch.load('./Desktop/Data_ML/test_DEPTH.pt')))\n",
        "test_ids = torch.tensor(np.array(torch.load('./Desktop/Data_ML/test_ID.pt')))"
      ],
      "metadata": {
        "id": "Due2ixEe1xoQ"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_merge = torch.cat((test_imgs, test_depth.reshape([849, 3, 224, 224])), 1)"
      ],
      "metadata": {
        "id": "iOrxt0L4-Y4x"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RGBDepth Predictions\n",
        "predictions_RGBD = []\n",
        "with torch.no_grad():\n",
        "    for i in range(len(test_merge)):\n",
        "        if (i%10 == 0):\n",
        "          print(i, end=\" \")\n",
        "        images = test_merge[i]\n",
        "        outputs = model_RGBD(images)\n",
        "        predictions_RGBD.append(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNeHzeCo-Vox",
        "outputId": "b7681b6e-95f9-4ffe-f255-25e68d4ff4b8"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RGBTop and Depth predictions\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for i in range(len(test_imgs)):\n",
        "        if (i%10 == 0):\n",
        "          print(i, end=\" \")\n",
        "        images = test_imgs[i]\n",
        "        outputs = model_RGBTop(images)\n",
        "        predictions.append(outputs)"
      ],
      "metadata": {
        "id": "iJVdyIFXseWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7557d787-8c86-4a10-b4da-ac9e603512d4"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 310 320 330 340 350 360 370 380 390 400 410 420 430 440 450 460 470 480 490 500 510 520 530 540 550 560 570 580 590 600 610 620 630 640 650 660 670 680 690 700 710 720 730 740 750 760 770 780 790 800 810 820 830 840 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "outfile = './Desktop/Data_ML/submission.csv'\n",
        "\n",
        "output_file = open(outfile, 'w')\n",
        "\n",
        "titles = ['ID', 'FINGER_POS_1', 'FINGER_POS_2', 'FINGER_POS_3', 'FINGER_POS_4', 'FINGER_POS_5', 'FINGER_POS_6',\n",
        "         'FINGER_POS_7', 'FINGER_POS_8', 'FINGER_POS_9', 'FINGER_POS_10', 'FINGER_POS_11', 'FINGER_POS_12']\n",
        "\n",
        "df = pd.concat([pd.DataFrame(np.array(test_ids)), pd.DataFrame.from_records(np.array(predictions_RGBD)).astype(float)], axis = 1, names = titles)\n",
        "df.columns = titles\n",
        "df.to_csv(outfile, index = False)\n",
        "print(\"Written to csv file {}\".format(outfile))"
      ],
      "metadata": {
        "id": "7T8gAzXP0Dtg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d782e74a-d368-4c1f-e250-29e4edc7df59"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Written to csv file ./Desktop/Data_ML/submission.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/var/folders/01/4rw9wbtn40s22slt7887wq7r0000gn/T/ipykernel_32605/2504348550.py:11: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "  df = pd.concat([pd.DataFrame(np.array(test_ids)), pd.DataFrame.from_records(np.array(predictions_RGBD)).astype(float)], axis = 1, names = titles)\n",
            "/var/folders/01/4rw9wbtn40s22slt7887wq7r0000gn/T/ipykernel_32605/2504348550.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  df = pd.concat([pd.DataFrame(np.array(test_ids)), pd.DataFrame.from_records(np.array(predictions_RGBD)).astype(float)], axis = 1, names = titles)\n"
          ]
        }
      ]
    }
  ]
}